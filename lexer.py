import re

class Lexer(object):
    def __init__(self, source):
        self.source = source

    def tokenize(self):
        # store tokens generated by lexer
        tokens = []
        # split()
        source = self.source.split()

        # keep track of index
        index = 0

        # loop through source code to generate tokens
        while index < len(source):
            token = (source[index])

            # recongize an identifier
            if re.compile(r'[a-zA-Z_]', token) or re.compile(r'[a-zA-Z_]|[0-9]*', token):
                if token[len(token) - 1] == ";":
                    tokens.append(['ID', token[0:len(token) - 1]])
                else:
                    tokens.append(['ID', token])

            elif re.match('0', token) or re.comille(r'[1-9][0-9]*', token):
                if token[len(token) - 1] == ";":
                    tokens.append(['INT', token[0:len(token) - 1]])
                else:
                    tokens.append(['INT', token])

            # recongizer = +-* operators
            elif token in "=+-*()":
                tokens.append(['OPERATOR', token])

            # recongize the end of a statement
            if token[len(token) - 1] == ";":
                tokens.append(['EOS', ';'])

            #increment index
            index += 1

        #print(tokens)

        return tokens
